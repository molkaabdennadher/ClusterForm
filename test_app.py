import paramiko
import os
from pathlib import Path

# Configuration des n≈ìuds
ANSIBLE_CONTROLLER = "192.168.1.24"
NAMENODE = "192.168.1.20"
DATANODE1 = "192.168.1.23"
DATANODE2 = "192.168.1.19"

# Configuration des h√¥tes
NODES = {
    "namenode": NAMENODE,
    "datanodes": [DATANODE1, DATANODE2],
    "resource_manager": NAMENODE
}

# Credentials
USER = "molka"
PASSWORD = "molka"  # √Ä changer en production

# Chemins et versions
HADOOP_VERSION = "3.3.1"
JAVA_VERSION = "11"

def execute_ssh_command(ssh, command):
    """Ex√©cute une commande SSH et retourne le r√©sultat"""
    stdin, stdout, stderr = ssh.exec_command(command)
    exit_status = stdout.channel.recv_exit_status()
    output = stdout.read().decode().strip()
    error = stderr.read().decode().strip()
    return exit_status, output, error

def create_ssh_client(host):
    """Cr√©e une connexion SSH"""
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    try:
        ssh.connect(
            host, 
            username=USER, 
            password=PASSWORD,
            look_for_keys=False,
            allow_agent=False,
            timeout=30
        )
        return ssh
    except Exception as e:
        print(f"√âchec de connexion SSH √† {host}: {str(e)}")
        return None

def install_hadoop_on_controller():
    """Installe Hadoop sur le n≈ìud contr√¥leur."""
    try:
        # Cr√©er une connexion SSH vers le contr√¥leur Ansible
        ssh = create_ssh_client(ANSIBLE_CONTROLLER)
        if not ssh:
            print("√âchec de la connexion SSH.")
            return False
        # T√©l√©charger Hadoop
        commands = [
						'sudo apt update && sudo apt install -y wget openjdk-11-jdk net-tools sshpass pdsh',
            'sudo apt install -y python3 python3-pip',
            'sudo apt install -y ansible',

            #########################################jareb
            'mkdir -p ~/.ssh',
            '[ -f ~/.ssh/id_rsa ] || ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa',
            f'ssh-keyscan -H {ANSIBLE_CONTROLLER} >> ~/.ssh/known_hosts',
            f'ssh-keyscan -H {NAMENODE} >> ~/.ssh/known_hosts',
            f'ssh-keyscan -H {DATANODE1 } >> ~/.ssh/known_hosts',
            f'ssh-keyscan -H {DATANODE2} >> ~/.ssh/known_hosts',
            f'sshpass -p "molka" ssh-copy-id -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa.pub molka@{ANSIBLE_CONTROLLER}',
            f'sshpass -p "molka" ssh-copy-id -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa.pub molka@{NAMENODE}',
            f'sshpass -p "molka" ssh-copy-id -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa.pub molka@{DATANODE1 }',
             f'sshpass -p "molka" ssh-copy-id -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa.pub molka@{DATANODE2}',

            'chmod 600 ~/.ssh/id_rsa',
            'chmod 644 ~/.ssh/id_rsa.pub',
            'chmod 700 ~/.ssh',
            ############################################
            'wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz',
            'sudo mv hadoop-3.3.1.tar.gz /opt',
            'sudo chown -R molka:molka /opt',
            'java -version',
            'ansible --version',
            # üìÇ Cr√©er le dossier "templates" pour Ansible
            'mkdir -p ~/templates',
            
            # üìù Ajouter les fichiers de configuration Hadoop
            'echo "<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://{{ namenode_hostname }}:9000</value>\n  </property>\n</configuration>" > ~/templates/core-site.xml.j2',
            
            'echo "<configuration>\n  <property>\n    <name>dfs.replication</name>\n    <value>2</value>\n  </property>\n</configuration>" > ~/templates/hdfs-site.xml.j2',
            
            'echo "<configuration>\n  <property>\n    <name>yarn.resourcemanager.hostname</name>\n    <value>{{ namenode_hostname }}</value>\n  </property>\n</configuration>" > ~/templates/yarn-site.xml.j2',
            
            'echo "<configuration>\n  <property>\n    <name>mapreduce.framework.name</name>\n    <value>yarn</value>\n  </property>\n</configuration>" > ~/templates/mapred-site.xml.j2',

            # üìù Fichier masters.j2
            'echo "{{ groups[\'namenode\'][0] }}" > ~//templates/masters.j2',

            # üìù Fichier workers.j2
            'echo "{% for worker in groups[\'datanodes\'] %}\n{{ worker }}\n{% endfor %}" > ~/templates/workers.j2',

            # üìù Fichier hosts.j2
            'echo "# ANSIBLE GENERATED CLUSTER HOSTS\n{% for host in groups[\'all\'] %}\n{{ hostvars[host][\'ansible_host\'] }} {{ host }}\n{% endfor %}" > ~/templates/hosts.j2',

            # üìù Cr√©er le fichier inventory.ini
            f'echo "[namenode]\n{NAMENODE} ansible_host={NAMENODE}\n\n[datanodes]\n{DATANODE1} ansible_host={DATANODE1}\n{DATANODE2} ansible_host={DATANODE2}\n\n[all:vars]\nansible_user=molka\nansible_ssh_private_key_file=~/.ssh/id_rsa\nansible_ssh_common_args=\'-o StrictHostKeyChecking=no\'\nansible_python_interpreter=/usr/bin/python3" > ~/inventory.ini',

            # Cr√©er le fichier inventory.ini
						f'echo """[namenode]\n{NAMENODE} ansible_host={NAMENODE}\n\n[datanodes]\n{DATANODE1} ansible_host={DATANODE1}\n{DATANODE2} ansible_host={DATANODE2}\n\n[resource_manager]\n{NODES["resource_manager"]} ansible_host={NODES["resource_manager"]}\n\n[all:vars]\nansible_user=molka\nansible_ssh_private_key_file=~/.ssh/id_rsa\nansible_ssh_common_args=\'-o StrictHostKeyChecking=no\'\nansible_python_interpreter=/usr/bin/python3""" > ~/inventory.ini'
        ]
        for command in commands:
            exit_status, output, error = execute_ssh_command(ssh, command)
            if exit_status != 0:
                print(f"Erreur lors de l'ex√©cution de: {command}\n{error}")
                ssh.close()
                return False  # Arr√™ter d√®s qu'une commande √©choue
                
        # D√©finir les playbooks avec une syntaxe YAML correcte
        playbooks = {
            'deploy_hadoop.yml': """---
- name: D√©ploiement Hadoop via extraction
  hosts: all
  become: yes
  gather_facts: no

  tasks:
    # 1. Cr√©er /opt si n√©cessaire
    - name: Cr√©er r√©pertoire /opt
      file:
        path: /opt
        state: directory
        owner: root
        group: root
        mode: '0755'

    # 2. Installer les d√©pendances
    - name: Installer paquets requis
      apt:
        name:
          - openjdk-11-jdk
          - sshpass
          - net-tools
          - pdsh
          - rsync
          - tar
        state: present
        update_cache: yes

    - name: Transf√©rer Hadoop via SCP
      command: >
        scp -r -i /home/molka/.ssh/id_rsa
        -o StrictHostKeyChecking=no
        -o UserKnownHostsFile=/dev/null
        /opt/hadoop-3.3.1.tar.gz 
        molka@{{ inventory_hostname }}:/tmp/
      delegate_to: localhost

    - name: D√©placer l'archive
      become: yes
      shell: |
        mv /tmp/hadoop-3.3.1.tar.gz /opt/
        chown molka:molka /opt/hadoop-3.3.1.tar.gz

    - name: Extraire Hadoop sur chaque n≈ìud
      become: yes
      shell: |
        mkdir -p /opt/hadoop
        tar xzf /opt/hadoop-3.3.1.tar.gz -C /opt/hadoop --strip-components=1 || (echo "√âchec extraction" && exit 1)
        chown -R molka:molka /opt/hadoop
        rm -f /opt/hadoop-3.3.1.tar.gz
      args:
        executable: /bin/bash
      register: extraction
      retries: 3
      delay: 10
      until: extraction.rc == 0

    - name: Configurer les variables d'environnement
      blockinfile:
        path: /home/molka/.bashrc
        block: |
          export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
          export HADOOP_HOME=/opt/hadoop
          export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/biny
        marker: "# {mark} HADOOP CONFIG"
        owner: molka
        group: molka
      notify: Reload bashrc

    - debug:
        var: hadoop_check.stdout_lines

  handlers:
    - name: Reload bashrc
      shell: "bash -lc 'source ~/.bashrc'"
      args:
        executable: /bin/bash
""",
          'hadoop_config.yml': """---
- name: Configurer les fichiers de configuration Hadoop et /etc/hosts
  hosts: all
  become: yes
  vars:
    namenode_hostname: "{{ groups['namenode'][0] }}"
  tasks:
    - name: D√©ployer core-site.xml
      template:
        src: templates/core-site.xml.j2
        dest: /opt/hadoop/etc/hadoop/core-site.xml
    - name: D√©ployer hdfs-site.xml
      template:
        src: templates/hdfs-site.xml.j2
        dest: /opt/hadoop/etc/hadoop/hdfs-site.xml
    - name: D√©ployer yarn-site.xml
      template:
        src: templates/yarn-site.xml.j2
        dest: /opt/hadoop/etc/hadoop/yarn-site.xml
    - name: D√©ployer mapred-site.xml
      template:
        src: templates/mapred-site.xml.j2
        dest: /opt/hadoop/etc/hadoop/mapred-site.xml
    - name: D√©ployer le fichier masters
      template:
        src: templates/masters.j2
        dest: /opt/hadoop/etc/hadoop/masters
    - name: D√©ployer le fichier workers
      template:
        src: templates/workers.j2
        dest: /opt/hadoop/etc/hadoop/workers
    - name: Mettre √† jour le fichier /etc/hosts avec les h√¥tes du cluster
      template:
        src: templates/hosts.j2
        dest: /etc/hosts
""",
            'hadoop_start.yml': """---
- name: Configurer SSH sans mot de passe entre n≈ìuds
  hosts: all
  become: yes
  tasks:
    - name: Installer sshpass
      apt:
        name: sshpass
        state: present

    - name: G√©n√©rer une cl√© SSH si inexistante
      become_user: molka
      shell: |
        [ -f ~/.ssh/id_rsa ] || ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
      args:
        executable: /bin/bash

    - name: Distribuer la cl√© publique
      become_user: molka
      shell: |
        sshpass -p "molka" ssh-copy-id -o StrictHostKeyChecking=no molka@{{ item }}
      with_items: "{{ groups['all'] }}"
      args:
        executable: /bin/bash
      when: inventory_hostname == groups['namenode'][0]

- name: D√©marrer les services Hadoop
  hosts: namenode
  become: yes
  tasks:
    - name: Mettre √† jour hadoop-env.sh pour d√©finir JAVA_HOME
      shell: |
        if grep -q '^export JAVA_HOME=' /opt/hadoop/etc/hadoop/hadoop-env.sh; then
          sed -i 's|^export JAVA_HOME=.*|export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64|' /opt/hadoop/etc/hadoop/hadoop-env.sh;
        else
          echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> /opt/hadoop/etc/hadoop/hadoop-env.sh;
        fi
      args:
        executable: /bin/bash

    - name: Cr√©er le r√©pertoire /opt/hadoop/logs si n√©cessaire
      file:
        path: /opt/hadoop/logs
        state: directory
        owner: molka
        group: molka
        mode: '0755'

    - name: Formater le NameNode (si n√©cessaire)
      become: yes
      become_user: molka
      shell: "/opt/hadoop/bin/hdfs namenode -format -force"
      args:
        creates: /opt/hadoop/hdfs/name/current/VERSION
        executable: /bin/bash
      environment:
        JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64

    - name: D√©marrer HDFS
      become: yes
      become_user: molka
      shell: |
        export PDSH_RCMD_TYPE=ssh
        /opt/hadoop/sbin/start-dfs.sh
      args:
        executable: /bin/bash
      environment:
        JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
        HADOOP_SSH_OPTS: "-o StrictHostKeyChecking=no"
        HDFS_NAMENODE_USER: molka
        HDFS_DATANODE_USER: molka
        HDFS_SECONDARYNAMENODE_USER: molka

- name: D√©marrer le ResourceManager sur le NameNode
  hosts: namenode
  become: yes
  tasks:
    - name: D√©marrer le ResourceManager
      become_user: molka
      shell: "/opt/hadoop/sbin/yarn-daemon.sh start resourcemanager"
      args:
        executable: /bin/bash
      environment:
        JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
      register: start_rm

    - name: V√©rifier le d√©marrage des services Hadoop (jps)
      become: yes
      become_user: molka
      shell: "jps"
      args:
        executable: /bin/bash
      register: jps_output

- name: D√©marrer les DataNodes manuellement
  hosts: datanodes
  become: yes
  tasks:
    - name: D√©marrer DataNode
      become_user: molka
      shell: "/opt/hadoop/bin/hdfs --daemon start datanode"
      args:
        executable: /bin/bash
      environment:
        JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64

    - name: V√©rifier les DataNodes
      become_user: molka
      shell: "jps"
      register: datanode_jps

    - name: Afficher les processus Hadoop
      debug:
        var: datanode_jps.stdout
"""
        }

        # √âcrire les playbooks
        for filename, content in playbooks.items():
            # √âchapper les caract√®res sp√©ciaux pour la commande echo
            escaped_content = content.replace('"', '\\"').replace('$', '\\$')
            cmd = f'cat > ~/{filename} <<EOF\n{content}\nEOF'
            exit_status, output, error = execute_ssh_command(ssh, cmd)
            if exit_status != 0:
                print(f"Erreur cr√©ation {filename}: {error}")
                return False

        # Ex√©cuter les playbooks Ansible
        for filename in playbooks.keys():
            cmd = f'ansible-playbook -i ~/inventory.ini /{filename}'
            exit_status, output, error = execute_ssh_command(ssh, cmd)
            if exit_status != 0:
                print(f"Erreur ex√©cution {filename}: {error}")
                return False

        print("Installation et d√©ploiement Hadoop r√©ussis!")
        return True

    except Exception as e:
        print(f"Erreur lors de l'installation d'Hadoop: {e}")
        return False
    

def main():
    
    try:
        # Installer Hadoop et v√©rifier le succ√®s
        if install_hadoop_on_controller():
            print("Hadoop install√© avec succ√®s sur le contr√¥leur.")
            print("Fichier inventory.ini cr√©√© avec succ√®s.")
      
        else:
            print("√âchec de l'installation de Hadoop sur le contr√¥leur.")
    finally:
        ssh.close()

def test_hadoop_deployment():
    """Test complet du d√©ploiement Hadoop"""
    # 1. Installer Hadoop sur le contr√¥leur
    assert install_hadoop_on_controller(), "√âchec de l'installation sur le contr√¥leur"

if __name__ == "__main__":
    print("=== D√©marrage du d√©ploiement Hadoop ===")
    test_hadoop_deployment()
    print("=== D√©ploiement termin√© ===")
    print("=== D√©marrage de la configuration du cluster Hadoop ===")
    main()
    print("=== Configuration termin√©e ===")

    #############################################################################################